{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/dpandya/miniconda3/envs/simple-diar/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "from simple_diarizer.diarizer import Diarizer\n",
    "from simple_diarizer.utils import (check_wav_16khz_mono, convert_wavfile,\n",
    "                                   waveplot, combined_waveplot, waveplot_perspeaker)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio, display, HTML\n",
    "from tqdm.autonotebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dpandya/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio file to single channel WAV using ffmpeg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/tmp/build/80754af9/ffmpeg_1587154242452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Guessed Channel Layout for Input Stream #0.0 : stereo\n",
      "Input #0, wav, from '../audioData/familyDecisionVids/336.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:05:27.97, bitrate: 512 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, stereo, s16, 512 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '../audioData/familyDecisionVids/336_converted.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.29.100\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 pcm_s16le\n",
      "size=   10249kB time=00:05:27.97 bitrate= 256.0kbits/s speed=3.96e+03x    \n",
      "video:0kB audio:10249kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000743%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running VAD...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m wav_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../audioData/familyDecisionVids/336.wav\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m diar \u001b[39m=\u001b[39m Diarizer(\n\u001b[1;32m      4\u001b[0m     embed_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mecapa\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     cluster_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msc\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     window\u001b[39m=\u001b[39m\u001b[39m1.5\u001b[39m,\n\u001b[1;32m      7\u001b[0m     period\u001b[39m=\u001b[39m\u001b[39m0.75\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m segments \u001b[39m=\u001b[39m diar\u001b[39m.\u001b[39;49mdiarize(wav_file,\n\u001b[1;32m     11\u001b[0m                         num_speakers\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#outfile='diar_rttn.rttm')\u001b[39;00m\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/simple-diar/lib/python3.11/site-packages/simple_diarizer/diarizer.py:250\u001b[0m, in \u001b[0;36mDiarizer.diarize\u001b[0;34m(self, wav_file, num_speakers, threshold, silence_tolerance, enhance_sim, extra_info, outfile)\u001b[0m\n\u001b[1;32m    247\u001b[0m     signal, fs \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(converted_wavfile)\n\u001b[1;32m    249\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning VAD...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m speech_ts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvad(signal[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    251\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSplitting by silence found \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m utterances\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(speech_ts)))\n\u001b[1;32m    252\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(speech_ts) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any speech during VAD\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/simple-diar/lib/python3.11/site-packages/simple_diarizer/diarizer.py:70\u001b[0m, in \u001b[0;36mDiarizer.vad\u001b[0;34m(self, signal)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvad\u001b[39m(\u001b[39mself\u001b[39m, signal):\n\u001b[1;32m     67\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    Runs the VAD model on the signal\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_speech_ts(signal, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvad_model)\n",
      "File \u001b[0;32m~/.cache/torch/hub/snakers4_silero-vad_master/utils_vad.py:267\u001b[0m, in \u001b[0;36mget_speech_timestamps\u001b[0;34m(audio, model, threshold, sampling_rate, min_speech_duration_ms, max_speech_duration_s, min_silence_duration_ms, window_size_samples, speech_pad_ms, return_seconds, visualize_probs, progress_tracking_callback)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(chunk) \u001b[39m<\u001b[39m window_size_samples:\n\u001b[1;32m    266\u001b[0m     chunk \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mpad(chunk, (\u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m(window_size_samples \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(chunk))))\n\u001b[0;32m--> 267\u001b[0m speech_prob \u001b[39m=\u001b[39m model(chunk, sampling_rate)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    268\u001b[0m speech_probs\u001b[39m.\u001b[39mappend(speech_prob)\n\u001b[1;32m    269\u001b[0m \u001b[39m# caculate progress and seng it to callback function\u001b[39;00m\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/simple-diar/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wav_file = '../audioData/familyDecisionVids/336.wav'\n",
    "\n",
    "diar = Diarizer(\n",
    "    embed_model='ecapa',\n",
    "    cluster_method='sc',\n",
    "    window=1.5,\n",
    "    period=0.75\n",
    ")\n",
    "\n",
    "segments = diar.diarize(wav_file,\n",
    "                        num_speakers=3)\n",
    "#outfile='diar_rttn.rttm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 3.266, 'end': 16.1579375, 'label': 1, 'start_sample': 52256, 'end_sample': 258527}\n",
      "{'start': 19.074, 'end': 20.199, 'label': 1, 'start_sample': 305184, 'end_sample': 323184}\n",
      "{'start': 20.199, 'end': 20.9259375, 'label': 0, 'start_sample': 323184, 'end_sample': 334815}\n",
      "{'start': 21.506, 'end': 23.9339375, 'label': 0, 'start_sample': 344096, 'end_sample': 382943}\n",
      "{'start': 24.162, 'end': 34.287, 'label': 0, 'start_sample': 386592, 'end_sample': 548592}\n",
      "{'start': 34.287, 'end': 36.537, 'label': 1, 'start_sample': 548592, 'end_sample': 584592}\n",
      "{'start': 36.537, 'end': 61.287, 'label': 2, 'start_sample': 584592, 'end_sample': 980592}\n",
      "{'start': 61.287, 'end': 68.5771875, 'label': 1, 'start_sample': 980592, 'end_sample': 1097235}\n"
     ]
    }
   ],
   "source": [
    "for i in segments:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple-diar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
